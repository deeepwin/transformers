{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author:** Deeepwin, Jason Brownlee [(Machine Learning Mastery)](https://machinelearningmastery.com)   \n",
    "**Date created:** 04.11.2022 <br>\n",
    "**Last modified:** 05.11.2022 <br>\n",
    "**Description:** An implementation of a Transfromer architecture from scratch using Keras and Tensorflow\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import math, matmul, reshape, shape, transpose, cast, float32, linalg, ones, maximum, newaxis\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, LayerNormalization, Layer, ReLU, Dropout, TextVectorization, Embedding, Input\n",
    " \n",
    "from keras.backend import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=160)\n",
    "\n",
    "# disable GPU\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads       = 8         # Number of self-attention heads\n",
    "d_k             = 64        # Dimensionality of the linearly projected queries and keys\n",
    "d_v             = 64        # Dimensionality of the linearly projected values\n",
    "dense_dim       = 2048      # Dimensionality of the inner fully connected layer\n",
    "embed_dim       = 512       # Dimensionality of the model sub-layers' outputs\n",
    "num_layers      = 2         # Number of layers in the encoder stack\n",
    " \n",
    "dropout_rate    = 0.1       # Frequency of dropping the input units in the dropout layers\n",
    "\n",
    "vocab_size      = 50        # Vocabulary size\n",
    "sequence_length = 4        # Maximum length of the input sequence\n",
    "\n",
    "batch_size      = 32        # Batch size from the training process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"600\" src=\"pics/score_matrix.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may note that the scaled dot-product attention can also apply a mask to the attention scores before feeding them into the softmax function. \n",
    "\n",
    "Since the word embeddings are zero-padded to a specific sequence length, a ***padding mask needs to be introduced in order to prevent the zero tokens from being processed*** along with the input in both the encoder and decoder stages. Furthermore, a look-ahead mask is also required to prevent the decoder from attending to succeeding words, such that the prediction for a particular word can only depend on known outputs for the words that come before it.\n",
    "\n",
    "These look-ahead and padding masks are applied inside the scaled dot-product attention set to -infinity all the values in the input to the softmax function that should not be considered. For each of these large negative inputs, the softmax function will, in turn, produce an output value that is close to zero, effectively masking them out. The use of these masks will become clearer when you progress to the implementation of the encoder and decoder blocks in separate tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"460\" src=\"pics/dot_product.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Link](https://machinelearningmastery.com/how-to-implement-scaled-dot-product-attention-from-scratch-in-tensorflow-and-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention masking is the feature that allows the transformer to not use recursion. Each possible sequence is masked and processed in parallel. For each of the sequences we have the shifted target vector as ground truth (y_true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"460\" src=\"pics/masking.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Scaled-Dot Product Attention\n",
    "class DotProductAttention(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "    \n",
    "        super(DotProductAttention, self).__init__(**kwargs)\n",
    " \n",
    "    def call(self, queries, keys, values, d_k, mask=None):                                  # queries = keys = values = (32, 8, 20, 8), all same values\n",
    "                                                                                            # mask = (32, 1, 20, 20)\n",
    "        # Scoring the queries against the keys after transposing the latter, and scaling    \n",
    "        scores = matmul(queries, keys, transpose_b=True) / math.sqrt(cast(d_k, float32))    # (32, 8, 20, 20), multiplication on last two dimensions if rank >= 2 \n",
    "                                                                                            # computing matrix dot product (20, 8) * (8, 20) = (20, 20)\n",
    "\n",
    "        # Apply mask to the attention scores. The mask will contain either 0 values to indicate that the corresponding token \n",
    "        # in the input sequence should be considered in the computations or a 1 to indicate otherwise. The mask will be multiplied \n",
    "        # by -1e9 to set the 1 values to large negative numbers subsequently applied to the attention scores:                                            \n",
    "        if mask is not None:            # (32, 8, 20, 20) as example with sequence_length=4, calculation is as following:\n",
    "            scores += -1e9 * mask       # mask                (-1e9*mask)                             (scores + -1e9*mask)\n",
    "                                        # [[0., 1., 1., 1.],  [[-0.e+00, -1.e+09, -1.e+09, -1.e+09],  [[ 5.4317653e-01, -1.0000000e+09, -1.0000000e+09, -1.0000000e+09],\n",
    "                                        #  [0., 0., 1., 1.],   [-0.e+00, -0.e+00, -1.e+09, -1.e+09],   [ 6.7063296e-01,  5.7927263e-01, -1.0000000e+09, -1.0000000e+09],\n",
    "                                        #  [0., 0., 0., 1.],   [-0.e+00, -0.e+00, -0.e+00, -1.e+09],   [ 8.5004723e-01,  7.3447722e-01,  6.0847449e-01, -1.0000000e+09],\n",
    "                                        #  [0., 0., 0., 0.]]   [-0.e+00, -0.e+00, -0.e+00, -0.e+00]]   [ 9.2180628e-01,  7.9571950e-01,  6.6288424e-01,  5.6855118e-01]]\n",
    "        weights = softmax(scores)                                                           # (32, 8, 20, 20)\n",
    " \n",
    "        # Computing the attention by a weighted sum of the value vectors\n",
    "        return matmul(weights, values)                                                      # (32, 8, 20, 8), computing (20, 20) * (20, 8) = (20, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img width=\"600\" src=\"pics/multi_head.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will be reshaping the linearly projected queries, keys, and values in such a manner as to allow the attention heads to be computed in parallel. This is different than shown in picture above.\n",
    "\n",
    "The queries, keys, and values will be fed as input into the multi-head attention block having a shape of (batch size, sequence length, model dimensionality), where the batch size is a hyperparameter of the training process, the sequence length defines the maximum length of the input/output phrases, and the model dimensionality is the dimensionality of the outputs produced by all sub-layers of the model. They are then passed through the respective dense layer to be linearly projected to a shape of (batch size, sequence length, queries/keys/values dimensionality).\n",
    "\n",
    "The linearly projected queries, keys, and values will be rearranged into (batch size, number of heads, sequence length, depth), by first reshaping them into (batch size, sequence length, number of heads, depth) and then transposing the second and third dimensions. For this purpose, you will create the class method, reshape_tensor, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Multi-Head Attention\n",
    "class MultiHeadAttention(Layer):\n",
    "\n",
    "    def __init__(self, num_heads, d_k, d_v, embed_dim, **kwargs):\n",
    "    \n",
    "        super(MultiHeadAttention, self).__init__(**kwargs)\n",
    "\n",
    "        self.attention = DotProductAttention()      # Scaled dot product attention\n",
    "        self.heads = num_heads                      # Number of attention heads to use\n",
    "        self.d_k = d_k                              # Dimensionality of the linearly projected queries and keys\n",
    "        self.d_v = d_v                              # Dimensionality of the linearly projected values\n",
    "        self.embed_dim = embed_dim                  # Dimensionality of the model\n",
    "        self.W_q = Dense(d_k)                       # Learned projection matrix for the queries\n",
    "        self.W_k = Dense(d_k)                       # Learned projection matrix for the keys\n",
    "        self.W_v = Dense(d_v)                       # Learned projection matrix for the values\n",
    "        self.W_o = Dense(embed_dim)                 # Learned projection matrix for the multi-head output\n",
    " \n",
    "    def reshape_tensor(self, x, heads, flag):\n",
    "        if flag:\n",
    "            # Tensor shape after reshaping and transposing: (batch_size, heads, seq_length, -1)\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], heads, -1))\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "        else:\n",
    "            # Reverting the reshaping and transposing operations: (batch_size, seq_length, d_k)\n",
    "            x = transpose(x, perm=(0, 2, 1, 3))\n",
    "            x = reshape(x, shape=(shape(x)[0], shape(x)[1], self.d_k))\n",
    "        return x\n",
    " \n",
    "    def call(self, queries, keys, values, mask=None):                           # queries = keys = values = (32, 20, 512), all same values\n",
    "                                                                                # mask = (32, 1, 20, 20)\n",
    "        # Rearrange the queries to be able to compute all heads in parallel     \n",
    "        out = self.W_q(queries)                                                 # (32, 20, 64), dense layer with (512,64) kernel operating along \n",
    "                                                                                #               axis 2 which means kernel is share between word embeddings (20)\n",
    "        q_reshaped = self.reshape_tensor(out, self.heads, True)                 # (32, 8, 20, 8]\n",
    "        \n",
    "        # Rearrange the keys to be able to compute all heads in parallel\n",
    "        out = self.W_k(keys)                                                    # (32, 20, 64)\n",
    "        k_reshaped = self.reshape_tensor(out, self.heads, True)                 # (32, 8, 20, 8]\n",
    " \n",
    "        # Rearrange the values to be able to compute all heads in parallel\n",
    "        out = self.W_v(values)                                                  # (32, 20, 64)\n",
    "        v_reshaped = self.reshape_tensor(out, self.heads, True)                 # (32, 8, 20, 8]\n",
    " \n",
    "        # Compute the multi-head attention output using the reshaped queries, keys and values\n",
    "        o_reshaped = self.attention(q_reshaped, k_reshaped, v_reshaped, self.d_k, mask) # (32, 8, 20, 8]\n",
    " \n",
    "        # Rearrange back the output into concatenated form\n",
    "        out = self.reshape_tensor(o_reshaped, self.heads, False)                # (32, 20, 8]\n",
    " \n",
    "        # Apply one final linear projection to the output to generate the multi-head attention\n",
    "        out = self.W_o(out)                                                     # (32, 20, 512]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Link](https://machinelearningmastery.com/how-to-implement-multi-head-attention-from-scratch-in-tensorflow-and-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4, 512)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from numpy import random\n",
    " \n",
    "queries     = random.random((batch_size, sequence_length, d_k))\n",
    "keys        = random.random((batch_size, sequence_length, d_k))\n",
    "values      = random.random((batch_size, sequence_length, d_v))\n",
    " \n",
    "output = MultiHeadAttention(num_heads, d_k, d_v, embed_dim)(queries, keys, values);\n",
    "output.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Embedding layer converts integers to dense vectors. This layer maps these integers to random numbers, which are later tuned during the training phase.\n",
    "\n",
    "We use two embeddings, one from the TextVectorization (word to integer) and one for the position indices. In a transformer model, the final output is the sum of both the word embeddings and the position embeddings. The sum is calculated element-wise.\n",
    "        \n",
    "See [(Link)](https://machinelearningmastery.com/the-transformer-positional-encoding-layer-in-keras-part-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionEmbeddingFixedWeights(Layer):\n",
    "\n",
    "    def __init__(self, sequence_length, vocab_size, output_dim, **kwargs):\n",
    "    \n",
    "        super(PositionEmbeddingFixedWeights, self).__init__(**kwargs)\n",
    "\n",
    "        word_embedding_matrix = self.get_position_encoding(vocab_size, output_dim)   \n",
    "        position_embedding_matrix = self.get_position_encoding(sequence_length, output_dim)     \n",
    "\n",
    "        # Embedding for the text vectorization\n",
    "        self.word_embedding_layer = Embedding(\n",
    "            input_dim=vocab_size, output_dim=output_dim,\n",
    "            weights=[word_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "        # Embedding for the indices (positions)\n",
    "        self.position_embedding_layer = Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim,\n",
    "            weights=[position_embedding_matrix],\n",
    "            trainable=False\n",
    "        )\n",
    "             \n",
    "    def get_position_encoding(self, seq_len, d, n=10000):\n",
    "        P = np.zeros((seq_len, d))\n",
    "        for k in range(seq_len):\n",
    "            for i in np.arange(int(d/2)):\n",
    "                denominator = np.power(n, 2*i/d)\n",
    "                P[k, 2*i] = np.sin(k/denominator)\n",
    "                P[k, 2*i+1] = np.cos(k/denominator)\n",
    "        return P\n",
    " \n",
    "    def call(self, inputs):                                                  # inputs = (32, 20)\n",
    "        position_indices = tf.range(tf.shape(inputs)[-1])                    # (20), vector with values 0 ... 19\n",
    "        embedded_words = self.word_embedding_layer(inputs)                   # (32, 20, 512)\n",
    "        embedded_indices = self.position_embedding_layer(position_indices)   # (20, 512)\n",
    "        return embedded_words + embedded_indices                             # (32, 20, 512) embeddings value are added element-wise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  ['', '[UNK]', 'robot', 'you', 'too', 'i', 'am', 'a']\n",
      "Vectorized words:  [[5 6 7 2]\n",
      " [3 4 2 0]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [[\"I am a robot\"], [\"you too robot\"]]\n",
    "sentence_data = tf.data.Dataset.from_tensor_slices(sentences)\n",
    "\n",
    "# Create the TextVectorization layer\n",
    "vectorize_layer = TextVectorization(max_tokens=vocab_size,\n",
    "                                    output_sequence_length=sequence_length)\n",
    "\n",
    "# Train the layer to create a dictionary\n",
    "vectorize_layer.adapt(sentence_data)\n",
    "\n",
    "# Convert all sentences to tensors\n",
    "word_tensors = tf.convert_to_tensor(sentences, dtype=tf.string)\n",
    "\n",
    "# Use the word tensors to get vectorized phrases\n",
    "vectorized_words = vectorize_layer(word_tensors)\n",
    "print(\"Vocabulary: \", vectorize_layer.get_vocabulary())\n",
    "print(\"Vectorized words: \", vectorized_words.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 4, 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, sequence_length)\n",
    "output = embedding(vectorized_words)\n",
    "output.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Add & Norm Layer\n",
    "class AddNormalization(Layer):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "    \n",
    "        super(AddNormalization, self).__init__(**kwargs)\n",
    "        self.layer_norm = LayerNormalization()  # Layer normalization layer\n",
    " \n",
    "    def call(self, x, sublayer_x):\n",
    "    \n",
    "        # The sublayer input and output need to be of the same shape to be summed\n",
    "        add = x + sublayer_x\n",
    " \n",
    "        # Apply layer normalization to the sum\n",
    "        return self.layer_norm(add)\n",
    " \n",
    "# Implementing the Feed-Forward Layer\n",
    "class FeedForward(Layer):\n",
    "    \n",
    "    def __init__(self, dense_dim, embed_dim, **kwargs):\n",
    "        super(FeedForward, self).__init__(**kwargs)\n",
    "        self.fully_connected1 = Dense(dense_dim)    # First fully connected layer\n",
    "        self.fully_connected2 = Dense(embed_dim)    # Second fully connected layer\n",
    "        self.activation = ReLU()                    # ReLU activation layer\n",
    " \n",
    "    def call(self, x):\n",
    "        \n",
    "        # The input is passed into the two fully-connected layers, with a ReLU in between\n",
    "        x_fc1 = self.fully_connected1(x)\n",
    " \n",
    "        return self.fully_connected2(self.activation(x_fc1))\n",
    " \n",
    "# Implementing the Encoder Layer\n",
    "class EncoderLayer(Layer):\n",
    "    def __init__(self, sequence_length, num_heads, d_k, d_v, embed_dim, dense_dim, rate, **kwargs):\n",
    "        super(EncoderLayer, self).__init__(**kwargs)\n",
    "        self.build(input_shape=[None, sequence_length, embed_dim])\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.multihead_attention = MultiHeadAttention(num_heads, d_k, d_v, embed_dim)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(dense_dim, embed_dim)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    " \n",
    "    def build_graph(self):\n",
    "        input_layer = Input(shape=(self.sequence_length, self.embed_dim))\n",
    "        return Model(inputs=[input_layer], outputs=self.call(input_layer, None, True))\n",
    "\n",
    "    def call(self, x, padding_mask, training):\n",
    "\n",
    "        # Multi-head attention layer\n",
    "        multihead_output = self.multihead_attention(x, x, x, padding_mask)\n",
    "        # Expected output shape = (batch_size, sequence_length, embed_dim)\n",
    " \n",
    "        # Add in a dropout layer\n",
    "        multihead_output = self.dropout1(multihead_output, training=training)\n",
    " \n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output = self.add_norm1(x, multihead_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, embed_dim)\n",
    " \n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output)\n",
    "        # Expected output shape = (batch_size, sequence_length, embed_dim)\n",
    " \n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout2(feedforward_output, training=training)\n",
    " \n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm2(addnorm_output, feedforward_output)\n",
    " \n",
    "# Implementing the Encoder\n",
    "class Encoder(Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, sequence_length, num_heads, d_k, d_v, embed_dim, dense_dim, num_layers, rate, **kwargs):\n",
    "\n",
    "        super(Encoder, self).__init__(**kwargs)\n",
    "        \n",
    "        self.pos_encoding   = PositionEmbeddingFixedWeights(sequence_length, vocab_size, embed_dim)\n",
    "        self.dropout        = Dropout(rate)\n",
    "        self.encoder_layer  = [EncoderLayer(sequence_length, num_heads, d_k, d_v, embed_dim, dense_dim, rate) for _ in range(num_layers)]\n",
    " \n",
    "    def call(self, input_sentence, padding_mask, training):\n",
    "        \n",
    "        # Generate the positional encoding\n",
    "        pos_encoding_output = self.pos_encoding(input_sentence)\n",
    "        # Expected output shape = (batch_size, sequence_length, embed_dim)\n",
    " \n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)\n",
    " \n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.encoder_layer):\n",
    "            x = layer(x, padding_mask, training)\n",
    " \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Link](https://machinelearningmastery.com/implementing-the-transformer-encoder-from-scratch-in-tensorflow-and-keras)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_input = random.random((batch_size, sequence_length))\n",
    "enc_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4, 512)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(vocab_size, sequence_length, num_heads, d_k, d_v, embed_dim, dense_dim, num_layers, dropout_rate)\n",
    "output = encoder(enc_input, None, True)\n",
    "output.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing the Decoder Layer\n",
    "class DecoderLayer(Layer):\n",
    "    def __init__(self, sequence_length, num_heads, d_k, d_v, embed_dim, dense_dim, rate, **kwargs):\n",
    "        \n",
    "        super(DecoderLayer, self).__init__(**kwargs)\n",
    "\n",
    "        self.build(input_shape=[None, sequence_length, embed_dim])\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.multihead_attention1 = MultiHeadAttention(num_heads, d_k, d_v, embed_dim)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.add_norm1 = AddNormalization()\n",
    "        self.multihead_attention2 = MultiHeadAttention(num_heads, d_k, d_v, embed_dim)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "        self.add_norm2 = AddNormalization()\n",
    "        self.feed_forward = FeedForward(dense_dim, embed_dim)\n",
    "        self.dropout3 = Dropout(rate)\n",
    "        self.add_norm3 = AddNormalization()\n",
    "\n",
    "    def build_graph(self):\n",
    "        input_layer = Input(shape=(self.sequence_length, self.embed_dim))\n",
    "        return Model(inputs=[input_layer], outputs=self.call(input_layer, input_layer, None, None, True))\n",
    "        \n",
    "    def call(self, x, encoder_output, lookahead_mask, padding_mask, training):\n",
    "\n",
    "        # Multi-head attention layer                                            # x = encoder_output = (32, 20, 512)\n",
    "                                                                                # lookahead_mask = (32, 1, 20, 20)\n",
    "                                                                                # padding_mask = (32, 1, 1, 20)\n",
    "        multihead_output1 = self.multihead_attention1(x, x, x, lookahead_mask)  # (32, 20, 512)\n",
    "\n",
    "        # Add in a dropout layer\n",
    "        multihead_output1 = self.dropout1(multihead_output1, training=training) # (32, 20, 512)\n",
    " \n",
    "        # Followed by an Add & Norm layer\n",
    "        addnorm_output1 = self.add_norm1(x, multihead_output1)                  # (32, 20, 512)\n",
    "\n",
    "        # Followed by another multi-head attention layer                        \n",
    "        multihead_output2 = self.multihead_attention2(addnorm_output1,          # (32, 20, 512)\n",
    "                                                      encoder_output, \n",
    "                                                      encoder_output, \n",
    "                                                      padding_mask)\n",
    "        # Add in another dropout layer\n",
    "        multihead_output2 = self.dropout2(multihead_output2, training=training) # (32, 20, 512)\n",
    " \n",
    "        # Followed by another Add & Norm layer\n",
    "        addnorm_output2 = self.add_norm1(addnorm_output1, multihead_output2)    # (32, 20, 512)\n",
    " \n",
    "        # Followed by a fully connected layer\n",
    "        feedforward_output = self.feed_forward(addnorm_output2)                 # (32, 20, 512)\n",
    "\n",
    " \n",
    "        # Add in another dropout layer\n",
    "        feedforward_output = self.dropout3( feedforward_output,                 # (32, 20, 512) \n",
    "                                            training=training) \n",
    " \n",
    "        # Followed by another Add & Norm layer\n",
    "        return self.add_norm3(addnorm_output2, feedforward_output)              # (32, 20, 512) \n",
    " \n",
    "# Implementing the Decoder\n",
    "class Decoder(Layer):\n",
    "\n",
    "    def __init__(self, vocab_size, sequence_length, num_heads, d_k, d_v, embed_dim, dense_dim, num_layers, rate, **kwargs):\n",
    "    \n",
    "        super(Decoder, self).__init__(**kwargs)\n",
    "    \n",
    "        self.pos_encoding = PositionEmbeddingFixedWeights(sequence_length, vocab_size, embed_dim)\n",
    "        self.dropout = Dropout(rate)\n",
    "        self.decoder_layer = [DecoderLayer(sequence_length, num_heads, d_k, d_v, embed_dim, dense_dim, rate) for _ in range(num_layers)]\n",
    " \n",
    "    def call(self, output_target, encoder_output, lookahead_mask, padding_mask, training):\n",
    "\n",
    "        # Generate the positional encoding                                          # output_target = (32, 20)\n",
    "        pos_encoding_output = self.pos_encoding(output_target)                      # (32, 20, 512)\n",
    " \n",
    "        # Add in a dropout layer\n",
    "        x = self.dropout(pos_encoding_output, training=training)                    # (32, 20, 512)\n",
    " \n",
    "        # Pass on the positional encoded values to each encoder layer\n",
    "        for i, layer in enumerate(self.decoder_layer):                              # encoder_output = (32, 20, 512)\n",
    "                                                                                    # lookahead_mask = (32, 1, 20, 20)\n",
    "            x = layer(x, encoder_output, lookahead_mask, padding_mask, training)    # (32, 20, 512)\n",
    " \n",
    "        return x                                                                    # (32, 20, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Link](https://machinelearningmastery.com/implementing-the-transformer-decoder-from-scratch-in-tensorflow-and-keras)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4, 512)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_input = random.random((batch_size, sequence_length))\n",
    "dec_input.shape\n",
    "enc_output = random.random((batch_size, sequence_length, embed_dim))\n",
    "enc_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 4, 512)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size, sequence_length, num_heads, d_k, d_v, embed_dim, dense_dim, num_layers, dropout_rate)\n",
    "output = decoder(dec_input, enc_output, None, True)\n",
    "output.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerModel(Model):\n",
    "    \n",
    "    def __init__(self, vocab_size, sequence_length, num_heads, d_k, d_v, embed_dim, d_ff_inner, num_layers, rate, **kwargs):\n",
    "\n",
    "        super(TransformerModel, self).__init__(**kwargs)\n",
    " \n",
    "        # Set up the encoder\n",
    "        self.encoder = Encoder(vocab_size, sequence_length, num_heads, d_k, d_v, embed_dim, d_ff_inner, num_layers, rate)\n",
    " \n",
    "        # Set up the decoder\n",
    "        self.decoder = Decoder(vocab_size, sequence_length, num_heads, d_k, d_v, embed_dim, d_ff_inner, num_layers, rate)\n",
    " \n",
    "        # Define the final dense layer\n",
    "        self.model_last_layer = Dense(vocab_size)\n",
    " \n",
    "    def call(self, encoder_input, decoder_input, training):                         # encoder_input = decoder_input = (32, 20), training = False\n",
    " \n",
    "        # Create padding mask to mask the encoder inputs and the encoder outputs in the decoder\n",
    "        enc_padding_mask = self.padding_mask(encoder_input)                         # (32, 1, 1, 20)\n",
    " \n",
    "        # Create and combine padding and look-ahead masks to be fed into the decoder\n",
    "        dec_in_padding_mask = self.padding_mask(decoder_input)                      # (32, 1, 1, 20)\n",
    "        dec_in_lookahead_mask = self.lookahead_mask(decoder_input.shape[1])         # (20, 20)   \n",
    "        dec_in_lookahead_mask = maximum(dec_in_padding_mask, dec_in_lookahead_mask) # (32, 1, 20, 20), returns maximum value element-wise\n",
    "                                                                                    #                  masks get broadcasted along column, example:\n",
    "                                                                                    # padding               lookahead               max()\n",
    "                                                                                    # [[[0., 0., 1., 0.]]]  [[[0., 1., 1., 1.],     [[0., 1., 1., 1.],\n",
    "                                                                                    #                         [0., 0., 1., 1.],      [0., 0., 1., 1.],\n",
    "                                                                                    #                         [0., 0., 0., 1.],      [0., 0., 1., 1.],\n",
    "                                                                                    #                         [0., 0., 0., 0.]]]     [0., 0., 1., 0.]]\n",
    "        # Feed the input into the encoder\n",
    "        encoder_output = self.encoder(encoder_input, enc_padding_mask, training)\n",
    " \n",
    "        # Feed the encoder output into the decoder\n",
    "        decoder_output = self.decoder(decoder_input, encoder_output, dec_in_lookahead_mask, enc_padding_mask, training) # (32, 20, 512)\n",
    " \n",
    "        # Pass the decoder output through a final dense layer\n",
    "        model_output = self.model_last_layer(decoder_output)                        # (32, 20, 50)\n",
    " \n",
    "        return model_output\n",
    "\n",
    "    def padding_mask(self, input):                                                  # input = (32, 20)\n",
    "\n",
    "        # Create mask which marks the zero padding values in the input by a 1.0\n",
    "        mask = math.equal(input, 0)                                                 # returns boolean value of (x == y) element-wise, finds 0\n",
    "        mask = cast(mask, float32)                                                  # (32, 20)\n",
    " \n",
    "        # The shape of the mask should be broadcastable to the shape\n",
    "        # of the attention weights that it will be masking later on\n",
    "        return mask[:, newaxis, newaxis, :]                                         # (32, 1, 1, 20)\n",
    " \n",
    "    def lookahead_mask(self, shape):                                                # shape = 20\n",
    "        \n",
    "        # Mask out future entries by marking them with a 1.0\n",
    "        mask = 1 - linalg.band_part(ones((shape, shape)), -1, 0)                    # creates upper triangle tensor with 0 diagonal, example shape=4 \n",
    "                                                                                    #   [[0., 1., 1., 1.],\n",
    "                                                                                    #    [0., 0., 1., 1.],\n",
    "                                                                                    #    [0., 0., 0., 1.],\n",
    "                                                                                    #    [0., 0., 0., 0.]]\n",
    "        return mask                                                                 # (20, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: [Link](https://machinelearningmastery.com/joining-the-transformer-encoder-and-decoder-and-masking/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build transformer model\n",
    "transformer = TransformerModel(vocab_size, sequence_length, num_heads, d_k, d_v, embed_dim, dense_dim, num_layers, dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = transformer(enc_input, dec_input)\n",
    "output.numpy().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('test')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8449b6f7e5d87e95487e64c6763915e36cc3912604d8f97c80e9659b35f7481b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
